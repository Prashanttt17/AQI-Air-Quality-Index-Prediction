
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQI Prediction Backend - Google Colab Notebook\n",
    "\n",
    "This notebook allows you to run the AQI Prediction backend in Google Colab, making it accessible to your frontend application.\n",
    "\n",
    "## Instructions\n",
    "1. Run each cell in order\n",
    "2. Copy the ngrok URL provided at the end to connect your frontend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up the environment\n",
    "\n",
    "First, we'll install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn pandas scikit-learn numpy python-multipart joblib pydantic python-dotenv requests fastapi-cors statsmodels matplotlib pmdarima pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create backend files\n",
    "\n",
    "Now we'll create the necessary backend files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "!mkdir -p models uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time_series_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile time_series_models.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "class TimeSeriesModels:\n",
    "    \"\"\"\n",
    "    Class containing implementation of various time series models\n",
    "    for AQI prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_data_for_ts(data_df: pd.DataFrame, target_col: str = 'aqi') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare data for time series modeling\n",
    "        \"\"\"\n",
    "        # Ensure date is in datetime format and set as index\n",
    "        df = data_df.copy()\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.sort_values('date')\n",
    "            df = df.set_index('date')\n",
    "        \n",
    "        # If we have very few data points, we cannot build a good model\n",
    "        # So we add some synthetic data points based on existing data trends\n",
    "        if len(df) < 14:\n",
    "            print(f\"Warning: Only {len(df)} data points provided. Adding synthetic data for better modeling.\")\n",
    "            \n",
    "            # Get the earliest date and add synthetic data before it\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                earliest_date = df.index.min()\n",
    "                \n",
    "                # Calculate average change between consecutive values\n",
    "                avg_change = 0\n",
    "                if len(df) > 1:\n",
    "                    changes = []\n",
    "                    for i in range(1, len(df)):\n",
    "                        changes.append(df.iloc[i][target_col] - df.iloc[i-1][target_col])\n",
    "                    avg_change = sum(changes) / len(changes) if changes else 0\n",
    "                \n",
    "                # Add synthetic data\n",
    "                synthetic_data = []\n",
    "                for i in range(1, 15 - len(df) + 1):\n",
    "                    synthetic_date = earliest_date - timedelta(days=i)\n",
    "                    \n",
    "                    # Calculate synthetic values using the average change (with some noise)\n",
    "                    base_val = df.iloc[0][target_col]\n",
    "                    synthetic_val = max(0, base_val - (avg_change * i) + np.random.normal(0, 5))\n",
    "                    \n",
    "                    # Create row with the same columns as the original data\n",
    "                    row = {col: 0 for col in df.columns}\n",
    "                    row[target_col] = synthetic_val\n",
    "                    \n",
    "                    synthetic_data.append((synthetic_date, row))\n",
    "                \n",
    "                # Add the synthetic data to the dataframe\n",
    "                for date, row in synthetic_data:\n",
    "                    df.loc[date] = row\n",
    "                \n",
    "                # Sort again after adding synthetic data\n",
    "                df = df.sort_index()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit_arima_model(data: pd.DataFrame, target_col: str = 'aqi', order: tuple = (5, 1, 0)):\n",
    "        \"\"\"\n",
    "        Fit an ARIMA model to the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time series data with date as index\n",
    "        target_col : str\n",
    "            Column name of the target variable\n",
    "        order : tuple\n",
    "            ARIMA order (p, d, q) parameters\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fitted_model : ARIMA model\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        df = TimeSeriesModels.prepare_data_for_ts(data, target_col)\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = ARIMA(df[target_col], order=order)\n",
    "        fitted_model = model.fit()\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(fitted_model, \"models/arima_model.pkl\")\n",
    "        \n",
    "        return fitted_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_arima(model, steps: int = 7):\n",
    "        \"\"\"\n",
    "        Generate predictions using the fitted ARIMA model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : ARIMA model\n",
    "            Fitted ARIMA model\n",
    "        steps : int\n",
    "            Number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        forecast : pd.Series\n",
    "            Forecast values\n",
    "        \"\"\"\n",
    "        forecast = model.forecast(steps=steps)\n",
    "        return forecast\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_arima_forecast(data: pd.DataFrame, target_col: str = 'aqi', forecast_steps: int = 7):\n",
    "        \"\"\"\n",
    "        Automatically find the best ARIMA parameters and forecast\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time series data with datetime index\n",
    "        target_col : str\n",
    "            Column name of the target variable\n",
    "        forecast_steps : int\n",
    "            Number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        forecast : pd.Series\n",
    "            Forecast values\n",
    "        model : ARIMA model\n",
    "            Fitted ARIMA model\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        df = TimeSeriesModels.prepare_data_for_ts(data, target_col)\n",
    "        \n",
    "        # Use auto_arima to find the best parameters\n",
    "        model = pm.auto_arima(\n",
    "            df[target_col],\n",
    "            seasonal=False,\n",
    "            stepwise=True,\n",
    "            suppress_warnings=True,\n",
    "            error_action=\"ignore\",\n",
    "            max_order=6,\n",
    "            trace=False\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        joblib.dump(model, \"models/auto_arima_model.pkl\")\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = model.predict(n_periods=forecast_steps)\n",
    "        \n",
    "        return forecast, model\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit_sarimax_model(data: pd.DataFrame, target_col: str = 'aqi', \n",
    "                          order: tuple = (1, 1, 1), seasonal_order: tuple = (1, 1, 1, 7)):\n",
    "        \"\"\"\n",
    "        Fit a SARIMAX model to the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time series data with date as index\n",
    "        target_col : str\n",
    "            Column name of the target variable\n",
    "        order : tuple\n",
    "            ARIMA order (p, d, q) parameters\n",
    "        seasonal_order : tuple\n",
    "            Seasonal order (P, D, Q, s) parameters\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fitted_model : SARIMAX model\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        df = TimeSeriesModels.prepare_data_for_ts(data, target_col)\n",
    "        \n",
    "        # If we have too few data points for seasonal modeling\n",
    "        if len(df) < seasonal_order[3]:\n",
    "            print(f\"Warning: Not enough data points for seasonal modeling with period={seasonal_order[3]}.\")\n",
    "            # Adjust the seasonal period\n",
    "            seasonal_order = (seasonal_order[0], seasonal_order[1], seasonal_order[2], min(7, len(df) // 2))\n",
    "            \n",
    "        # Fit SARIMAX model\n",
    "        model = SARIMAX(df[target_col], order=order, seasonal_order=seasonal_order)\n",
    "        \n",
    "        try:\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            \n",
    "            # Save the model\n",
    "            joblib.dump(fitted_model, \"models/sarimax_model.pkl\")\n",
    "            \n",
    "            return fitted_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting SARIMAX model: {e}\")\n",
    "            # Fall back to ARIMA if SARIMAX fails\n",
    "            print(\"Falling back to ARIMA model\")\n",
    "            arima_model = TimeSeriesModels.fit_arima_model(data, target_col)\n",
    "            return arima_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_sarimax(model, steps: int = 7):\n",
    "        \"\"\"\n",
    "        Generate predictions using the fitted SARIMAX model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : SARIMAX model\n",
    "            Fitted SARIMAX model\n",
    "        steps : int\n",
    "            Number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        forecast : pd.Series\n",
    "            Forecast values\n",
    "        \"\"\"\n",
    "        forecast = model.forecast(steps=steps)\n",
    "        return forecast\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_best_model_for_data(data: pd.DataFrame, target_col: str = 'aqi'):\n",
    "        \"\"\"\n",
    "        Determine the best model for the given data\n",
    "        \"\"\"\n",
    "        # If we have at least 14 data points, we can try SARIMAX\n",
    "        if len(data) >= 14:\n",
    "            try:\n",
    "                # Try SARIMAX model\n",
    "                return TimeSeriesModels.fit_sarimax_model(data, target_col)\n",
    "            except Exception as e:\n",
    "                print(f\"SARIMAX failed: {e}\")\n",
    "                # Fall back to auto ARIMA\n",
    "                try:\n",
    "                    _, model = TimeSeriesModels.auto_arima_forecast(data, target_col)\n",
    "                    return model\n",
    "                except Exception as e2:\n",
    "                    print(f\"Auto ARIMA failed: {e2}\")\n",
    "                    # Fall back to basic ARIMA\n",
    "                    return TimeSeriesModels.fit_arima_model(data, target_col)\n",
    "        else:\n",
    "            # For small datasets, auto ARIMA is better\n",
    "            try:\n",
    "                _, model = TimeSeriesModels.auto_arima_forecast(data, target_col)\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                print(f\"Auto ARIMA failed: {e}\")\n",
    "                # Fall back to basic ARIMA\n",
    "                return TimeSeriesModels.fit_arima_model(data, target_col)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(model_name: str):\n",
    "        \"\"\"\n",
    "        Load a saved model\n",
    "        \"\"\"\n",
    "        model_path = f\"models/{model_name}_model.pkl\"\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            return joblib.load(model_path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def forecast_with_model(model_name: str, data: pd.DataFrame, steps: int = 7, target_col: str = 'aqi'):\n",
    "        \"\"\"\n",
    "        Generate forecasts using the specified model\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        df = TimeSeriesModels.prepare_data_for_ts(data, target_col)\n",
    "        last_date = df.index[-1]\n",
    "        \n",
    "        # Load or fit model\n",
    "        model = TimeSeriesModels.load_model(model_name)\n",
    "        \n",
    "        if model is None:\n",
    "            # Model not found, fit a new one\n",
    "            if model_name.lower() == 'arima':\n",
    "                model = TimeSeriesModels.fit_arima_model(df, target_col)\n",
    "            elif model_name.lower() == 'sarimax':\n",
    "                model = TimeSeriesModels.fit_sarimax_model(df, target_col)\n",
    "            else:\n",
    "                # Default to auto ARIMA\n",
    "                _, model = TimeSeriesModels.auto_arima_forecast(df, target_col)\n",
    "        \n",
    "        # Generate forecast\n",
    "        try:\n",
    "            forecast = model.forecast(steps=steps)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating forecast with {model_name}: {e}\")\n",
    "            # Try to refit the model\n",
    "            if model_name.lower() == 'arima':\n",
    "                model = TimeSeriesModels.fit_arima_model(df, target_col)\n",
    "            elif model_name.lower() == 'sarimax':\n",
    "                model = TimeSeriesModels.fit_sarimax_model(df, target_col)\n",
    "            else:\n",
    "                # Default to auto ARIMA\n",
    "                _, model = TimeSeriesModels.auto_arima_forecast(df, target_col)\n",
    "            \n",
    "            forecast = model.forecast(steps=steps)\n",
    "        \n",
    "        # Convert forecast to DataFrame with dates\n",
    "        dates = [last_date + timedelta(days=i+1) for i in range(steps)]\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'date': dates,\n",
    "            target_col: forecast.values if hasattr(forecast, 'values') else forecast\n",
    "        })\n",
    "        \n",
    "        # Convert values to positive numbers and round\n",
    "        forecast_df[target_col] = forecast_df[target_col].apply(lambda x: max(0, round(x)))\n",
    "        \n",
    "        return forecast_df\n",
    "\n",
    "    @staticmethod\n",
    "    def process_csv_data(csv_file_path: str, target_col: str = 'aqi', date_col: str = 'date'):\n",
    "        \"\"\"\n",
    "        Process CSV data for time series modeling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if date_col not in df.columns:\n",
    "                raise ValueError(f\"Date column '{date_col}' not found in CSV file\")\n",
    "            if target_col not in df.columns:\n",
    "                raise ValueError(f\"Target column '{target_col}' not found in CSV file\")\n",
    "            \n",
    "            # Process date column\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "            \n",
    "            # Sort by date\n",
    "            df = df.sort_values(date_col)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV file: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI, HTTPException, Depends, Query, UploadFile, File\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from time_series_models import TimeSeriesModels\n",
    "\n",
    "# Models for request/response\n",
    "class PollutantData(BaseModel):\n",
    "    pm25: float\n",
    "    pm10: float\n",
    "    no2: float\n",
    "    o3: float\n",
    "    co: float\n",
    "    so2: float\n",
    "    nh3: float\n",
    "\n",
    "class AQIDataPoint(BaseModel):\n",
    "    date: str\n",
    "    city: str\n",
    "    location: Optional[str] = None\n",
    "    aqi: float\n",
    "    pollutants: Optional[PollutantData] = None\n",
    "    predicted: Optional[bool] = False\n",
    "\n",
    "class AQIRequest(BaseModel):\n",
    "    city: str\n",
    "    state: Optional[str] = None\n",
    "    country: Optional[str] = \"India\"\n",
    "    api_key: str\n",
    "    platform: str = \"airvisual\"  # 'airvisual' or 'aqicn'\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    historical_data: List[AQIDataPoint]\n",
    "    model_name: str = \"ARIMA\"  # Default to ARIMA if not specified\n",
    "\n",
    "class CSVDataRequest(BaseModel):\n",
    "    target_column: str = \"aqi\"\n",
    "    date_column: str = \"date\"\n",
    "    city_column: Optional[str] = None\n",
    "    model_name: str = \"ARIMA\"\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"AQI Prediction API\")\n",
    "\n",
    "# Add CORS middleware to allow requests from frontend\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allow all origins (you should restrict this in production)\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Create upload directory if it doesn't exist\n",
    "UPLOAD_DIR = \"uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# API Endpoints\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"AQI Prediction API is running\"}\n",
    "\n",
    "@app.post(\"/api/fetch-aqi\", response_model=List[AQIDataPoint])\n",
    "async def fetch_aqi_data(request: AQIRequest):\n",
    "    \"\"\"\n",
    "    Fetch AQI data from the selected platform (airvisual or aqicn)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if request.platform == \"airvisual\":\n",
    "            return await fetch_airvisual_data(request.city, request.state, request.country, request.api_key)\n",
    "        else:\n",
    "            return await fetch_aqicn_data(request.city, request.api_key)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error fetching AQI data: {str(e)}\")\n",
    "\n",
    "@app.post(\"/api/predict\", response_model=List[AQIDataPoint])\n",
    "async def predict_aqi(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Generate AQI predictions based on historical data and chosen model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert input data to pandas DataFrame for processing\n",
    "        data_points = []\n",
    "        for point in request.historical_data:\n",
    "            data_dict = {\n",
    "                \"date\": point.date,\n",
    "                \"city\": point.city,\n",
    "                \"aqi\": point.aqi\n",
    "            }\n",
    "            if point.pollutants:\n",
    "                data_dict.update({\n",
    "                    \"pm25\": point.pollutants.pm25,\n",
    "                    \"pm10\": point.pollutants.pm10,\n",
    "                    \"no2\": point.pollutants.no2,\n",
    "                    \"o3\": point.pollutants.o3,\n",
    "                    \"co\": point.pollutants.co,\n",
    "                    \"so2\": point.pollutants.so2,\n",
    "                    \"nh3\": point.pollutants.nh3\n",
    "                })\n",
    "            data_points.append(data_dict)\n",
    "            \n",
    "        df = pd.DataFrame(data_points)\n",
    "        \n",
    "        # Sort by date\n",
    "        if not df.empty:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df.sort_values('date')\n",
    "        \n",
    "        # Make predictions using appropriate model\n",
    "        predictions = generate_predictions(df, request.model_name)\n",
    "        \n",
    "        # Convert predictions back to AQIDataPoint format\n",
    "        result = []\n",
    "        for index, row in predictions.iterrows():\n",
    "            pollutants = None\n",
    "            if all(col in row.index for col in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]):\n",
    "                pollutants = PollutantData(\n",
    "                    pm25=float(row[\"pm25\"]),\n",
    "                    pm10=float(row[\"pm10\"]),\n",
    "                    no2=float(row[\"no2\"]),\n",
    "                    o3=float(row[\"o3\"]),\n",
    "                    co=float(row[\"co\"]),\n",
    "                    so2=float(row[\"so2\"]),\n",
    "                    nh3=float(row[\"nh3\"])\n",
    "                )\n",
    "            \n",
    "            result.append(AQIDataPoint(\n",
    "                date=row[\"date\"].strftime(\"%Y-%m-%d\") if isinstance(row[\"date\"], (datetime, pd.Timestamp)) else row[\"date\"],\n",
    "                city=row[\"city\"],\n",
    "                location=row[\"location\"] if \"location\" in row else None,\n",
    "                aqi=float(row[\"aqi\"]),\n",
    "                pollutants=pollutants,\n",
    "                predicted=bool(row[\"predicted\"])\n",
    "            ))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error generating predictions: {str(e)}\")\n",
    "\n",
    "@app.post(\"/api/predict-csv\", response_model=List[AQIDataPoint])\n",
    "async def predict_from_csv(\n",
    "    file: UploadFile = File(...),\n",
    "    target_column: str = Query(\"aqi\"),\n",
    "    date_column: str = Query(\"date\"),\n",
    "    city_column: Optional[str] = Query(None),\n",
    "    model_name: str = Query(\"ARIMA\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions from uploaded CSV data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the uploaded file\n",
    "        file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            content = await file.read()\n",
    "            f.write(content)\n",
    "        \n",
    "        # Process the CSV file\n",
    "        df = TimeSeriesModels.process_csv_data(file_path, target_column, date_column)\n",
    "        \n",
    "        if df is None:\n",
    "            raise HTTPException(status_code=400, detail=\"Failed to process CSV file\")\n",
    "        \n",
    "        # Add city information if provided\n",
    "        if city_column and city_column in df.columns:\n",
    "            city = df[city_column].iloc[0]\n",
    "        else:\n",
    "            city = \"Unknown\"\n",
    "            df['city'] = city\n",
    "        \n",
    "        # Generate predictions using the specified model\n",
    "        predictions_df = generate_predictions_from_csv(df, model_name, target_column)\n",
    "        \n",
    "        # Convert to AQIDataPoint format\n",
    "        result = []\n",
    "        for _, row in predictions_df.iterrows():\n",
    "            result.append(AQIDataPoint(\n",
    "                date=row[\"date\"].strftime(\"%Y-%m-%d\") if isinstance(row[\"date\"], (datetime, pd.Timestamp)) else row[\"date\"],\n",
    "                city=row[\"city\"] if \"city\" in row else city,\n",
    "                aqi=float(row[target_column]),\n",
    "                predicted=bool(row[\"predicted\"])\n",
    "            ))\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing CSV file: {str(e)}\")\n",
    "\n",
    "@app.get(\"/api/models\")\n",
    "async def list_available_models():\n",
    "    \"\"\"\n",
    "    List all available prediction models\n",
    "    \"\"\"\n",
    "    basic_models = [\"ARIMA\", \"SARIMAX\", \"RandomForest\", \"LSTM\"]\n",
    "    \n",
    "    # Check for custom trained models\n",
    "    model_files = [f.replace(\"_model.pkl\", \"\") for f in os.listdir(\"models\") if f.endswith(\"_model.pkl\")]\n",
    "    \n",
    "    # Combine lists and remove duplicates\n",
    "    all_models = list(set(basic_models + model_files))\n",
    "    \n",
    "    return {\"models\": all_models}\n",
    "\n",
    "# Helper functions for data fetching and predictions\n",
    "async def fetch_airvisual_data(city: str, state: Optional[str], country: str, api_key: str) -> List[AQIDataPoint]:\n",
    "    \"\"\"\n",
    "    Fetch data from AirVisual API\n",
    "    \"\"\"\n",
    "    # Define base URL and parameters\n",
    "    base_url = \"https://api.airvisual.com/v2/city\"\n",
    "    params = {\n",
    "        \"city\": city,\n",
    "        \"state\": state if state and state != \"All States\" else \"Delhi\",  # Default to Delhi if not specified\n",
    "        \"country\": country,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    \n",
    "    # Make API request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise HTTPException(status_code=response.status_code, \n",
    "                           detail=f\"AirVisual API error: {response.text}\")\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Process API response\n",
    "    if data[\"status\"] == \"success\":\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        city_name = city\n",
    "        \n",
    "        # Create current data point\n",
    "        current_aqi = data[\"data\"][\"current\"][\"pollution\"][\"aqius\"]\n",
    "        pollutants = PollutantData(\n",
    "            pm25=data[\"data\"][\"current\"][\"pollution\"].get(\"pm25\", 0),\n",
    "            pm10=data[\"data\"][\"current\"][\"pollution\"].get(\"pm10\", 0),\n",
    "            no2=0,  # AirVisual free API doesn't provide these values\n",
    "            o3=0,\n",
    "            co=0,\n",
    "            so2=0,\n",
    "            nh3=0\n",
    "        )\n",
    "        \n",
    "        current_point = AQIDataPoint(\n",
    "            date=current_date,\n",
    "            city=city_name,\n",
    "            aqi=current_aqi,\n",
    "            pollutants=pollutants,\n",
    "            predicted=False\n",
    "        )\n",
    "        \n",
    "        # Generate historical data (simulated)\n",
    "        result = [current_point]\n",
    "        for i in range(1, 15):\n",
    "            past_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            variation = np.random.randint(-10, 11)\n",
    "            historical_aqi = max(0, current_aqi + variation)\n",
    "            \n",
    "            pollutants_variation = {\n",
    "                \"pm25\": max(0, pollutants.pm25 + np.random.randint(-5, 6)),\n",
    "                \"pm10\": max(0, pollutants.pm10 + np.random.randint(-7, 8)),\n",
    "                \"no2\": 0,\n",
    "                \"o3\": 0,\n",
    "                \"co\": 0,\n",
    "                \"so2\": 0,\n",
    "                \"nh3\": 0\n",
    "            }\n",
    "            \n",
    "            result.append(AQIDataPoint(\n",
    "                date=past_date,\n",
    "                city=city_name,\n",
    "                aqi=historical_aqi,\n",
    "                pollutants=PollutantData(**pollutants_variation),\n",
    "                predicted=False\n",
    "            ))\n",
    "        \n",
    "        # Sort by date\n",
    "        result.sort(key=lambda x: x.date)\n",
    "        return result\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail=\"Failed to get data from AirVisual\")\n",
    "\n",
    "async def fetch_aqicn_data(city: str, api_key: str) -> List[AQIDataPoint]:\n",
    "    \"\"\"\n",
    "    Fetch data from AQICN API\n",
    "    \"\"\"\n",
    "    # Extract base city name for API query\n",
    "    if \",\" in city:\n",
    "        base_city = city.split(\",\")[-1].strip()\n",
    "    else:\n",
    "        base_city = city\n",
    "    \n",
    "    # Make API request\n",
    "    base_url = f\"https://api.waqi.info/feed/{base_city}/\"\n",
    "    params = {\"token\": api_key}\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise HTTPException(status_code=response.status_code, \n",
    "                           detail=f\"AQICN API error: {response.text}\")\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Process API response\n",
    "    if data[\"status\"] == \"ok\":\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Extract location info\n",
    "        full_location = data[\"data\"][\"city\"][\"name\"]\n",
    "        location_parts = full_location.split(\",\")\n",
    "        specific_location = location_parts[0].strip() if len(location_parts) > 1 else \"\"\n",
    "        \n",
    "        # Determine city from location\n",
    "        city_name = base_city\n",
    "        if len(location_parts) > 1:\n",
    "            city_name = location_parts[-1].strip()\n",
    "        \n",
    "        # Extract current AQI and pollutants\n",
    "        current_aqi = data[\"data\"][\"aqi\"]\n",
    "        iaqi = data[\"data\"][\"iaqi\"]\n",
    "        pollutants = PollutantData(\n",
    "            pm25=iaqi.get(\"pm25\", {}).get(\"v\", 0),\n",
    "            pm10=iaqi.get(\"pm10\", {}).get(\"v\", 0),\n",
    "            no2=iaqi.get(\"no2\", {}).get(\"v\", 0),\n",
    "            o3=iaqi.get(\"o3\", {}).get(\"v\", 0),\n",
    "            co=iaqi.get(\"co\", {}).get(\"v\", 0),\n",
    "            so2=iaqi.get(\"so2\", {}).get(\"v\", 0),\n",
    "            nh3=0  # AQICN doesn't provide NH3 typically\n",
    "        )\n",
    "        \n",
    "        # Create current data point\n",
    "        current_point = AQIDataPoint(\n",
    "            date=current_date,\n",
    "            city=city_name,\n",
    "            location=specific_location,\n",
    "            aqi=current_aqi,\n",
    "            pollutants=pollutants,\n",
    "            predicted=False\n",
    "        )\n",
    "        \n",
    "        # Generate historical data (simulated)\n",
    "        result = [current_point]\n",
    "        for i in range(1, 15):\n",
    "            past_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            variation = np.random.randint(-10, 11)\n",
    "            historical_aqi = max(0, current_aqi + variation)\n",
    "            \n",
    "            pollutants_variation = {\n",
    "                \"pm25\": max(0, pollutants.pm25 + np.random.randint(-5, 6)),\n",
    "                \"pm10\": max(0, pollutants.pm10 + np.random.randint(-7, 8)),\n",
    "                \"no2\": max(0, pollutants.no2 + np.random.randint(-4, 5)),\n",
    "                \"o3\": max(0, pollutants.o3 + np.random.randint(-3, 4)),\n",
    "                \"co\": max(0, pollutants.co + np.random.randint(-2, 3)),\n",
    "                \"so2\": max(0, pollutants.so2 + np.random.randint(-1, 2)),\n",
    "                \"nh3\": 0\n",
    "            }\n",
    "            \n",
    "            result.append(AQIDataPoint(\n",
    "                date=past_date,\n",
    "                city=city_name,\n",
    "                location=specific_location,\n",
    "                aqi=historical_aqi,\n",
    "                pollutants=PollutantData(**pollutants_variation),\n",
    "                predicted=False\n",
    "            ))\n",
    "        \n",
    "        # Sort by date\n",
    "        result.sort(key=lambda x: x.date)\n",
    "        return result\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail=\"Failed to get data from AQICN\")\n",
    "\n",
    "def generate_predictions(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate AQI predictions using the specified model\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"city\", \"location\", \"aqi\", \"predicted\"])\n",
    "    \n",
    "    # Extract basic info that we'll need for predictions\n",
    "    city = df.iloc[-1][\"city\"] \n",
    "    location = df.iloc[-1][\"location\"] if \"location\" in df.columns else None\n",
    "    \n",
    "    # Determine if we should use advanced time series models\n",
    "    use_time_series = model_name.upper() in [\"ARIMA\", \"SARIMAX\"]\n",
    "    \n",
    "    # Get the latest actual data point\n",
    "    current_date = datetime.now().date()\n",
    "    \n",
    "    # Find the latest non-predicted data point\n",
    "    actual_data = df[~df.get(\"predicted\", False)].copy() if \"predicted\" in df.columns else df.copy()\n",
    "    actual_data = actual_data.sort_values(\"date\", ascending=False)\n",
    "    \n",
    "    current_aqi_point = None\n",
    "    if not actual_data.empty:\n",
    "        current_aqi_point = actual_data.iloc[0].to_dict()\n",
    "    \n",
    "    # Prepare for predictions\n",
    "    forecast_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Generate 7-day forecast\n",
    "        forecast_dates = [current_date + timedelta(days=i) for i in range(7)]\n",
    "        \n",
    "        # Use time series models for ARIMA and SARIMAX\n",
    "        if use_time_series:\n",
    "            # Prepare data for time series modeling\n",
    "            ts_df = df[[\"date\", \"aqi\"]].copy()\n",
    "            ts_df['date'] = pd.to_datetime(ts_df['date'])\n",
    "            ts_df = ts_df.set_index('date')\n",
    "            \n",
    "            # Generate forecasts\n",
    "            if model_name.upper() == \"ARIMA\":\n",
    "                forecast = TimeSeriesModels.forecast_with_model(\"arima\", df, 7)\n",
    "            elif model_name.upper() == \"SARIMAX\":\n",
    "                forecast = TimeSeriesModels.forecast_with_model(\"sarimax\", df, 7)\n",
    "            else:\n",
    "                forecast = TimeSeriesModels.forecast_with_model(\"arima\", df, 7)  # Default\n",
    "                \n",
    "            # Create forecast DataFrame with city and predicted flag\n",
    "            forecast_df = forecast.copy()\n",
    "            forecast_df['city'] = city\n",
    "            forecast_df['predicted'] = True\n",
    "            \n",
    "            if location:\n",
    "                forecast_df['location'] = location\n",
    "                \n",
    "            # Ensure the first day's AQI matches current if available\n",
    "            if current_aqi_point and len(forecast_df) > 0:\n",
    "                # The first prediction will be for tomorrow, so we insert today's actual value\n",
    "                today_row = {\n",
    "                    'date': current_date,\n",
    "                    'aqi': current_aqi_point['aqi'],\n",
    "                    'city': city,\n",
    "                    'predicted': False\n",
    "                }\n",
    "                if location:\n",
    "                    today_row['location'] = location\n",
    "                \n",
    "                # Add today's row and resort\n",
    "                forecast_df = pd.concat([forecast_df, pd.DataFrame([today_row])])\n",
    "                forecast_df = forecast_df.sort_values('date')\n",
    "            \n",
    "        else:\n",
    "            # For other models, use the existing simulation-based approach\n",
    "            # Start with today's date\n",
    "            forecast_df = pd.DataFrame({\n",
    "                \"date\": forecast_dates,\n",
    "                \"city\": city,\n",
    "                \"predicted\": True\n",
    "            })\n",
    "            \n",
    "            if location:\n",
    "                forecast_df[\"location\"] = location\n",
    "            \n",
    "            # Use a different forecasting approach based on the model name\n",
    "            last_aqi = df.iloc[-1][\"aqi\"] if not df.empty else 100\n",
    "            aqi_values = []\n",
    "            \n",
    "            if model_name == \"RandomForest\":\n",
    "                # Simulate Random Forest-like behavior with step-wise predictions\n",
    "                for i in range(7):\n",
    "                    if i == 0 and current_aqi_point:\n",
    "                        # For today, use the actual current AQI\n",
    "                        aqi_values.append(current_aqi_point[\"aqi\"])\n",
    "                    else:\n",
    "                        # Each step is a bit less certain (increasing randomness)\n",
    "                        prev = aqi_values[-1] if aqi_values else last_aqi\n",
    "                        random_component = np.random.normal(0, 2 + i)\n",
    "                        aqi_values.append(max(0, prev * 0.9 + random_component))\n",
    "                        \n",
    "            elif model_name == \"LSTM\":\n",
    "                # Simulate LSTM-like behavior with trend and seasonality\n",
    "                for i in range(7):\n",
    "                    if i == 0 and current_aqi_point:\n",
    "                        # For today, use the actual current AQI\n",
    "                        aqi_values.append(current_aqi_point[\"aqi\"])\n",
    "                    else:\n",
    "                        # Simulate trend + seasonality + residual\n",
    "                        trend = -2  # Slight downward trend\n",
    "                        seasonality = 5 * np.sin(i/7 * 2 * np.pi)  # Weekly cycle\n",
    "                        residual = np.random.normal(0, 3)\n",
    "                        \n",
    "                        prev = aqi_values[-1] if aqi_values else last_aqi\n",
    "                        aqi_values.append(max(0, prev + trend + seasonality + residual))\n",
    "            else:\n",
    "                # Simple linear trend with noise\n",
    "                for i in range(7):\n",
    "                    if i == 0 and current_aqi_point:\n",
    "                        # For today, use the actual current AQI\n",
    "                        aqi_values.append(current_aqi_point[\"aqi\"])\n",
    "                    else:\n",
    "                        base = last_aqi - i * 2  # Linear decrease\n",
    "                        noise = np.random.normal(0, 5)\n",
    "                        aqi_values.append(max(0, base + noise))\n",
    "            \n",
    "            # Round AQI values\n",
    "            forecast_df[\"aqi\"] = [round(val) for val in aqi_values]\n",
    "        \n",
    "        # Generate pollutant predictions\n",
    "        if \"pollutants\" in df.columns or any(col in df.columns for col in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]):\n",
    "            # Get the latest pollutant values as base\n",
    "            latest_pollutants = {}\n",
    "            for pollutant in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]:\n",
    "                if pollutant in df.columns:\n",
    "                    latest_pollutants[pollutant] = df.iloc[-1].get(pollutant, 0)\n",
    "                else:\n",
    "                    latest_pollutants[pollutant] = 0\n",
    "            \n",
    "            # Add predictions for each pollutant\n",
    "            for pollutant in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]:\n",
    "                base_val = latest_pollutants[pollutant]\n",
    "                pollutant_vals = []\n",
    "                \n",
    "                for i in range(len(forecast_df)):\n",
    "                    if forecast_df.iloc[i].get('predicted', True) == False and pollutant in current_aqi_point:\n",
    "                        # For actual data points, use actual value if available\n",
    "                        pollutant_vals.append(current_aqi_point[pollutant])\n",
    "                    else:\n",
    "                        # Generate reasonable prediction based on base value and AQI trend\n",
    "                        aqi_ratio = forecast_df.iloc[i]['aqi'] / last_aqi if last_aqi > 0 else 1\n",
    "                        predicted_val = base_val * aqi_ratio * (0.95 + np.random.random() * 0.1)\n",
    "                        pollutant_vals.append(max(0, round(predicted_val)))\n",
    "                \n",
    "                forecast_df[pollutant] = pollutant_vals\n",
    "        \n",
    "        # Convert date column to string format if it's not already\n",
    "        if isinstance(forecast_df[\"date\"].iloc[0], (datetime, pd.Timestamp)):\n",
    "            forecast_df[\"date\"] = forecast_df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating predictions: {str(e)}\")\n",
    "        # Return empty dataframe if prediction fails\n",
    "        return pd.DataFrame(columns=[\"date\", \"city\", \"location\", \"aqi\", \"predicted\"])\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def generate_predictions_from_csv(df: pd.DataFrame, model_name: str, target_col: str = 'aqi') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate predictions from CSV data\n",
    "    \"\"\"\n",
    "    # Check if we have the minimum required columns\n",
    "    required_cols = ['date', target_col]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column {col} not found in CSV data\")\n",
    "    \n",
    "    # Add city column if it doesn't exist\n",
    "    if 'city' not in df.columns:\n",
    "        df['city'] = 'Unknown'\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Generate 7-day forecast using appropriate model\n",
    "    if model_name.upper() in [\"ARIMA\", \"SARIMAX\"]:\n",
    "        # Use time series model\n",
    "        forecast = TimeSeriesModels.forecast_with_model(\n",
    "            model_name.lower(), df, 7, target_col\n",
    "        )\n",
    "        \n",
    "        # Add prediction flag\n",
    "        forecast['predicted'] = True\n",
    "        \n",
    "        # Add city if it exists in original data\n",
    "        if 'city' in df.columns:\n",
    "            forecast['city'] = df['city'].iloc[0]\n",
    "        else:\n",
    "            forecast['city'] = 'Unknown'\n",
    "            \n",
    "        # Combine historical and forecast data\n",
    "        last_date = df['date'].max()\n",
    "        historical = df.copy()\n",
    "        historical['predicted'] = False\n",
    "        \n",
    "        # Only keep historical data up to the last date to avoid overlap\n",
    "        historical = historical[historical['date'] <= last_date]\n",
    "        \n",
    "        # Combine and sort\n",
    "        combined = pd.concat([historical, forecast])\n",
    "        combined = combined.sort_values('date')\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    else:\n",
    "        # For other models, use a simpler approach\n",
    "        # Get the last actual value\n",
    "        last_actual = df[target_col].iloc[-1]\n",
    "        \n",
    "        # Generate dates for next 7 days\n",
    "        last_date = df['date'].max()\n",
    "        forecast_dates = [last_date + timedelta(days=i+1) for i in range(7)]\n",
    "        \n",
    "        # Create forecast DataFrame\n",
    "        forecast = pd.DataFrame({'date': forecast_dates})\n",
    "        forecast['city'] = df['city'].iloc[0] if 'city' in df.columns else 'Unknown'\n",
    "        forecast['predicted'] = True\n",
    "        \n",
    "        # Generate values based on model type\n",
    "        if model_name.upper() == \"RANDOMFOREST\":\n",
    "            # Random forest tends to have less extreme predictions\n",
    "            vals = [max(0, last_actual * (0.95 + 0.1 * np.random.randn()) - i * 2) for i in range(7)]\n",
    "            forecast[target_col] = [round(val) for val in vals]\n",
    "        elif model_name.upper() == \"LSTM\":\n",
    "            # LSTM can capture patterns better\n",
    "            vals = []\n",
    "            prev = last_actual\n",
    "            for i in range(7):\n",
    "                # Add cyclical pattern + trend\n",
    "                val = prev * 0.9 + last_actual * 0.1 + 5 * np.sin(i/7 * 2 * np.pi) - i\n",
    "                vals.append(max(0, val + np.random.randn() * 5))\n",
    "                prev = val\n",
    "            forecast[target_col] = [round(val) for val in vals]\n",
    "        else:\n",
    "            # Default simple approach\n",
    "            vals = [max(0, last_actual - i * 3 + np.random.randn() * 7) for i in range(7)]\n",
    "            forecast[target_col] = [round(val) for val in vals]\n",
    "        \n",
    "        # Combine historical and forecast data\n",
    "        historical = df.copy()\n",
    "        historical['predicted'] = False\n",
    "        combined = pd.concat([historical, forecast])\n",
    "        combined = combined.sort_values('date')\n",
    "        \n",
    "        return combined\n",
    "\n",
    "# Run the server with: uvicorn main:app --reload\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the FastAPI server\n",
    "\n",
    "Now, let's run the FastAPI server using ngrok to make it publicly accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyngrok import ngrok, conf\n",
    "import threading\n",
    "import time\n",
    "import uvicorn\n",
    "\n",
    "# Start the FastAPI app in a separate thread\n",
    "def start_server():\n",
    "    uvicorn.run(\"main:app\", host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "# Start the server in a background thread\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for the server to start\n",
    "time.sleep(2)\n",
    "\n",
    "# Setup ngrok\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(f\"\\n\\nâœ… Backend server is running at: {ngrok_tunnel.public_url}\")\n",
    "print(\"\\nðŸ“‹ Use this URL in your frontend settings to connect to the backend\")\n",
    "print(\"\\nðŸ“ API Documentation: {}/docs\".format(ngrok_tunnel.public_url))\n",
    "print(\"\\nKeep this notebook running while using the backend!\\n\")\n",
    "\n",
    "# Keep the notebook running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down...\")\n",
    "    ngrok.disconnect(ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the API\n",
    "\n",
    "Once the server is running, you can test the endpoints using the URL provided above.\n",
    "\n",
    "### Example: Creating and Using a CSV for predictions\n",
    "\n",
    "Let's create a sample CSV file and test the prediction with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "dates = [datetime.now() - timedelta(days=i) for i in range(30)]\n",
    "dates = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "# Generate AQI values with seasonality and trend\n",
    "base_aqi = 100\n",
    "aqi_values = []\n",
    "for i in range(30):\n",
    "    # Add trend (slight decrease)\n",
    "    trend = -i * 0.5\n",
    "    \n",
    "    # Add weekly seasonality\n",
    "    season = 15 * np.sin(i/7 * 2 * np.pi)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 10)\n",
    "    \n",
    "    # Combine components\n",
    "    aqi = max(0, round(base_aqi + trend + season + noise))\n",
    "    aqi_values.append(aqi)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'city': 'Delhi',\n",
    "    'aqi': aqi_values\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = 'sample_aqi_data.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Created sample CSV file: {csv_path}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this backend with your frontend\n",
    "\n",
    "1. Copy the ngrok URL displayed above\n",
    "2. In your frontend application, open the Backend Settings\n",
    "3. Enable backend integration\n",
    "4. Paste the ngrok URL\n",
    "5. Test the connection\n",
    "6. Save settings\n",
    "\n",
    "Now your frontend will use this backend for AQI predictions using ARIMA and SARIMAX models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
