
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_cell"
      },
      "source": [
        "# AQI Prediction Backend - Google Colab Runner\n",
        "\n",
        "This notebook sets up and runs the FastAPI backend for the AQI Prediction application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "## 1. Setup the Environment\n",
        "\n",
        "First, we'll install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn pandas scikit-learn numpy python-multipart joblib pydantic python-dotenv requests fastapi-cors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_files_cell"
      },
      "source": [
        "## 2. Create Backend Files\n",
        "\n",
        "Now let's create the necessary backend files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_main_py"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, HTTPException, Depends, Query\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from typing import List, Optional, Dict, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Models for request/response\n",
        "class PollutantData(BaseModel):\n",
        "    pm25: float\n",
        "    pm10: float\n",
        "    no2: float\n",
        "    o3: float\n",
        "    co: float\n",
        "    so2: float\n",
        "    nh3: float\n",
        "\n",
        "class AQIDataPoint(BaseModel):\n",
        "    date: str\n",
        "    city: str\n",
        "    location: Optional[str] = None\n",
        "    aqi: float\n",
        "    pollutants: Optional[PollutantData] = None\n",
        "    predicted: Optional[bool] = False\n",
        "\n",
        "class AQIRequest(BaseModel):\n",
        "    city: str\n",
        "    state: Optional[str] = None\n",
        "    country: Optional[str] = \"India\"\n",
        "    api_key: str\n",
        "    platform: str = \"airvisual\"  # 'airvisual' or 'aqicn'\n",
        "\n",
        "class PredictionRequest(BaseModel):\n",
        "    historical_data: List[AQIDataPoint]\n",
        "    model_name: str = \"ARIMA\"  # Default to ARIMA if not specified\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"AQI Prediction API\")\n",
        "\n",
        "# Add CORS middleware to allow requests from frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all origins (you should restrict this in production)\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load ML models\n",
        "MODEL_PATH = \"models/\"\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "\n",
        "# Helper function to ensure models are loaded\n",
        "def load_model(model_name: str):\n",
        "    model_file = f\"{MODEL_PATH}{model_name.lower()}_model.pkl\"\n",
        "    \n",
        "    # For demonstration, we'll create dummy models if they don't exist\n",
        "    if not os.path.exists(model_file):\n",
        "        # In production, you'd want to train and save actual models\n",
        "        # For now, we'll just create a simple dummy model\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        model = LinearRegression()\n",
        "        model.fit(np.array([[1], [2], [3]]), np.array([10, 20, 30]))\n",
        "        joblib.dump(model, model_file)\n",
        "    \n",
        "    return joblib.load(model_file)\n",
        "\n",
        "# API Endpoints\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"AQI Prediction API is running\"}\n",
        "\n",
        "@app.post(\"/api/fetch-aqi\", response_model=List[AQIDataPoint])\n",
        "async def fetch_aqi_data(request: AQIRequest):\n",
        "    \"\"\"\n",
        "    Fetch AQI data from the selected platform (airvisual or aqicn)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if request.platform == \"airvisual\":\n",
        "            return await fetch_airvisual_data(request.city, request.state, request.country, request.api_key)\n",
        "        else:\n",
        "            return await fetch_aqicn_data(request.city, request.api_key)\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error fetching AQI data: {str(e)}\")\n",
        "\n",
        "@app.post(\"/api/predict\", response_model=List[AQIDataPoint])\n",
        "async def predict_aqi(request: PredictionRequest):\n",
        "    \"\"\"\n",
        "    Generate AQI predictions based on historical data and chosen model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert input data to pandas DataFrame for processing\n",
        "        data_points = []\n",
        "        for point in request.historical_data:\n",
        "            data_dict = {\n",
        "                \"date\": point.date,\n",
        "                \"city\": point.city,\n",
        "                \"aqi\": point.aqi\n",
        "            }\n",
        "            if point.location:\n",
        "                data_dict[\"location\"] = point.location\n",
        "            if point.pollutants:\n",
        "                data_dict.update({\n",
        "                    \"pm25\": point.pollutants.pm25,\n",
        "                    \"pm10\": point.pollutants.pm10,\n",
        "                    \"no2\": point.pollutants.no2,\n",
        "                    \"o3\": point.pollutants.o3,\n",
        "                    \"co\": point.pollutants.co,\n",
        "                    \"so2\": point.pollutants.so2,\n",
        "                    \"nh3\": point.pollutants.nh3\n",
        "                })\n",
        "            data_points.append(data_dict)\n",
        "            \n",
        "        df = pd.DataFrame(data_points)\n",
        "        \n",
        "        # Sort by date\n",
        "        if not df.empty:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "        \n",
        "        # Make predictions\n",
        "        predictions = generate_predictions(df, request.model_name)\n",
        "        \n",
        "        # Convert predictions back to AQIDataPoint format\n",
        "        result = []\n",
        "        for index, row in predictions.iterrows():\n",
        "            pollutants = None\n",
        "            if all(col in row.index for col in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]):\n",
        "                pollutants = PollutantData(\n",
        "                    pm25=float(row[\"pm25\"]),\n",
        "                    pm10=float(row[\"pm10\"]),\n",
        "                    no2=float(row[\"no2\"]),\n",
        "                    o3=float(row[\"o3\"]),\n",
        "                    co=float(row[\"co\"]),\n",
        "                    so2=float(row[\"so2\"]),\n",
        "                    nh3=float(row[\"nh3\"])\n",
        "                )\n",
        "            \n",
        "            result.append(AQIDataPoint(\n",
        "                date=row[\"date\"].strftime(\"%Y-%m-%d\") if isinstance(row[\"date\"], pd.Timestamp) else row[\"date\"],\n",
        "                city=row[\"city\"],\n",
        "                location=row[\"location\"] if \"location\" in row else None,\n",
        "                aqi=float(row[\"aqi\"]),\n",
        "                pollutants=pollutants,\n",
        "                predicted=bool(row[\"predicted\"])\n",
        "            ))\n",
        "        \n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error generating predictions: {str(e)}\")\n",
        "\n",
        "# Helper functions for data fetching and predictions\n",
        "async def fetch_airvisual_data(city: str, state: Optional[str], country: str, api_key: str) -> List[AQIDataPoint]:\n",
        "    \"\"\"\n",
        "    Fetch data from AirVisual API\n",
        "    \"\"\"\n",
        "    # Define base URL and parameters\n",
        "    base_url = \"https://api.airvisual.com/v2/city\"\n",
        "    params = {\n",
        "        \"city\": city,\n",
        "        \"state\": state if state and state != \"All States\" else \"Delhi\",  # Default to Delhi if not specified\n",
        "        \"country\": country,\n",
        "        \"key\": api_key\n",
        "    }\n",
        "    \n",
        "    # Make API request\n",
        "    response = requests.get(base_url, params=params)\n",
        "    \n",
        "    if not response.ok:\n",
        "        raise HTTPException(status_code=response.status_code, \n",
        "                           detail=f\"AirVisual API error: {response.text}\")\n",
        "    \n",
        "    data = response.json()\n",
        "    \n",
        "    # Process API response\n",
        "    if data[\"status\"] == \"success\":\n",
        "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        city_name = city\n",
        "        \n",
        "        # Create current data point\n",
        "        current_aqi = data[\"data\"][\"current\"][\"pollution\"][\"aqius\"]\n",
        "        pollutants = PollutantData(\n",
        "            pm25=data[\"data\"][\"current\"][\"pollution\"].get(\"pm25\", 0),\n",
        "            pm10=data[\"data\"][\"current\"][\"pollution\"].get(\"pm10\", 0),\n",
        "            no2=0,  # AirVisual free API doesn't provide these values\n",
        "            o3=0,\n",
        "            co=0,\n",
        "            so2=0,\n",
        "            nh3=0\n",
        "        )\n",
        "        \n",
        "        current_point = AQIDataPoint(\n",
        "            date=current_date,\n",
        "            city=city_name,\n",
        "            aqi=current_aqi,\n",
        "            pollutants=pollutants,\n",
        "            predicted=False\n",
        "        )\n",
        "        \n",
        "        # Generate historical data (simulated)\n",
        "        result = [current_point]\n",
        "        for i in range(1, 15):\n",
        "            past_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "            variation = np.random.randint(-10, 11)\n",
        "            historical_aqi = max(0, current_aqi + variation)\n",
        "            \n",
        "            pollutants_variation = {\n",
        "                \"pm25\": max(0, pollutants.pm25 + np.random.randint(-5, 6)),\n",
        "                \"pm10\": max(0, pollutants.pm10 + np.random.randint(-7, 8)),\n",
        "                \"no2\": 0,\n",
        "                \"o3\": 0,\n",
        "                \"co\": 0,\n",
        "                \"so2\": 0,\n",
        "                \"nh3\": 0\n",
        "            }\n",
        "            \n",
        "            result.append(AQIDataPoint(\n",
        "                date=past_date,\n",
        "                city=city_name,\n",
        "                aqi=historical_aqi,\n",
        "                pollutants=PollutantData(**pollutants_variation),\n",
        "                predicted=False\n",
        "            ))\n",
        "        \n",
        "        # Sort by date\n",
        "        result.sort(key=lambda x: x.date)\n",
        "        return result\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=\"Failed to get data from AirVisual\")\n",
        "\n",
        "async def fetch_aqicn_data(city: str, api_key: str) -> List[AQIDataPoint]:\n",
        "    \"\"\"\n",
        "    Fetch data from AQICN API\n",
        "    \"\"\"\n",
        "    # Extract base city name for API query\n",
        "    if \",\" in city:\n",
        "        base_city = city.split(\",\")[-1].strip()\n",
        "    else:\n",
        "        base_city = city\n",
        "    \n",
        "    # Make API request\n",
        "    base_url = f\"https://api.waqi.info/feed/{base_city}/\"\n",
        "    params = {\"token\": api_key}\n",
        "    \n",
        "    response = requests.get(base_url, params=params)\n",
        "    \n",
        "    if not response.ok:\n",
        "        raise HTTPException(status_code=response.status_code, \n",
        "                           detail=f\"AQICN API error: {response.text}\")\n",
        "    \n",
        "    data = response.json()\n",
        "    \n",
        "    # Process API response\n",
        "    if data[\"status\"] == \"ok\":\n",
        "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "        # Extract location info\n",
        "        full_location = data[\"data\"][\"city\"][\"name\"]\n",
        "        location_parts = full_location.split(\",\")\n",
        "        specific_location = location_parts[0].strip() if len(location_parts) > 1 else \"\"\n",
        "        \n",
        "        # Determine city from location\n",
        "        city_name = base_city\n",
        "        if len(location_parts) > 1:\n",
        "            city_name = location_parts[-1].strip()\n",
        "        \n",
        "        # Extract current AQI and pollutants\n",
        "        current_aqi = data[\"data\"][\"aqi\"]\n",
        "        iaqi = data[\"data\"][\"iaqi\"]\n",
        "        pollutants = PollutantData(\n",
        "            pm25=iaqi.get(\"pm25\", {}).get(\"v\", 0),\n",
        "            pm10=iaqi.get(\"pm10\", {}).get(\"v\", 0),\n",
        "            no2=iaqi.get(\"no2\", {}).get(\"v\", 0),\n",
        "            o3=iaqi.get(\"o3\", {}).get(\"v\", 0),\n",
        "            co=iaqi.get(\"co\", {}).get(\"v\", 0),\n",
        "            so2=iaqi.get(\"so2\", {}).get(\"v\", 0),\n",
        "            nh3=0  # AQICN doesn't provide NH3 typically\n",
        "        )\n",
        "        \n",
        "        # Create current data point\n",
        "        current_point = AQIDataPoint(\n",
        "            date=current_date,\n",
        "            city=city_name,\n",
        "            location=specific_location,\n",
        "            aqi=current_aqi,\n",
        "            pollutants=pollutants,\n",
        "            predicted=False\n",
        "        )\n",
        "        \n",
        "        # Generate historical data (simulated)\n",
        "        result = [current_point]\n",
        "        for i in range(1, 15):\n",
        "            past_date = (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
        "            variation = np.random.randint(-10, 11)\n",
        "            historical_aqi = max(0, current_aqi + variation)\n",
        "            \n",
        "            pollutants_variation = {\n",
        "                \"pm25\": max(0, pollutants.pm25 + np.random.randint(-5, 6)),\n",
        "                \"pm10\": max(0, pollutants.pm10 + np.random.randint(-7, 8)),\n",
        "                \"no2\": max(0, pollutants.no2 + np.random.randint(-4, 5)),\n",
        "                \"o3\": max(0, pollutants.o3 + np.random.randint(-3, 4)),\n",
        "                \"co\": max(0, pollutants.co + np.random.randint(-2, 3)),\n",
        "                \"so2\": max(0, pollutants.so2 + np.random.randint(-1, 2)),\n",
        "                \"nh3\": 0\n",
        "            }\n",
        "            \n",
        "            result.append(AQIDataPoint(\n",
        "                date=past_date,\n",
        "                city=city_name,\n",
        "                location=specific_location,\n",
        "                aqi=historical_aqi,\n",
        "                pollutants=PollutantData(**pollutants_variation),\n",
        "                predicted=False\n",
        "            ))\n",
        "        \n",
        "        # Sort by date\n",
        "        result.sort(key=lambda x: x.date)\n",
        "        return result\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=\"Failed to get data from AQICN\")\n",
        "\n",
        "def generate_predictions(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate AQI predictions using the specified model\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=[\"date\", \"city\", \"location\", \"aqi\", \"predicted\"])\n",
        "    \n",
        "    # Extract basic info that we'll need for predictions\n",
        "    city = df.iloc[-1][\"city\"] \n",
        "    location = df.iloc[-1][\"location\"] if \"location\" in df.columns else None\n",
        "    \n",
        "    # Get the latest actual data point\n",
        "    current_date = datetime.now().date()\n",
        "    current_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "    \n",
        "    # Find the latest non-predicted data point\n",
        "    actual_data = df[~df.get(\"predicted\", False)].copy() if \"predicted\" in df.columns else df.copy()\n",
        "    actual_data = actual_data.sort_values(\"date\", ascending=False)\n",
        "    \n",
        "    current_aqi_point = None\n",
        "    if not actual_data.empty:\n",
        "        current_aqi_point = actual_data.iloc[0].to_dict()\n",
        "    \n",
        "    # Prepare for predictions\n",
        "    forecast_df = pd.DataFrame()\n",
        "    \n",
        "    try:\n",
        "        # Here we would typically:\n",
        "        # 1. Pre-process data\n",
        "        # 2. Load the appropriate ML model\n",
        "        # 3. Make predictions\n",
        "        \n",
        "        # For simplicity, we'll simulate the forecast with a basic approach\n",
        "        # based on the model name\n",
        "        \n",
        "        # Start with today's date\n",
        "        forecast_dates = [current_date + timedelta(days=i) for i in range(7)]\n",
        "        forecast_df = pd.DataFrame({\n",
        "            \"date\": forecast_dates,\n",
        "            \"city\": city,\n",
        "            \"predicted\": True\n",
        "        })\n",
        "        \n",
        "        if location:\n",
        "            forecast_df[\"location\"] = location\n",
        "        \n",
        "        # Use a different forecasting approach based on the model name\n",
        "        last_aqi = df.iloc[-1][\"aqi\"] if not df.empty else 100\n",
        "        aqi_values = []\n",
        "        \n",
        "        if model_name == \"ARIMA\":\n",
        "            # Simulate ARIMA-like behavior with autoregression \n",
        "            for i in range(7):\n",
        "                if i == 0 and current_aqi_point:\n",
        "                    # For today, use the actual current AQI\n",
        "                    aqi_values.append(current_aqi_point[\"aqi\"])\n",
        "                else:\n",
        "                    # AR(1) process with some noise\n",
        "                    prev = aqi_values[-1] if aqi_values else last_aqi\n",
        "                    aqi_values.append(max(0, 0.8 * prev + np.random.normal(0, 5)))\n",
        "                    \n",
        "        elif model_name == \"LSTM\":\n",
        "            # Simulate LSTM-like behavior with trend and seasonality\n",
        "            for i in range(7):\n",
        "                if i == 0 and current_aqi_point:\n",
        "                    # For today, use the actual current AQI\n",
        "                    aqi_values.append(current_aqi_point[\"aqi\"])\n",
        "                else:\n",
        "                    # Simulate trend + seasonality + residual\n",
        "                    trend = -2  # Slight downward trend\n",
        "                    seasonality = 5 * np.sin(i/7 * 2 * np.pi)  # Weekly cycle\n",
        "                    residual = np.random.normal(0, 3)\n",
        "                    \n",
        "                    prev = aqi_values[-1] if aqi_values else last_aqi\n",
        "                    aqi_values.append(max(0, prev + trend + seasonality + residual))\n",
        "        \n",
        "        elif model_name == \"RandomForest\":\n",
        "            # Simulate Random Forest-like behavior with step-wise predictions\n",
        "            for i in range(7):\n",
        "                if i == 0 and current_aqi_point:\n",
        "                    # For today, use the actual current AQI\n",
        "                    aqi_values.append(current_aqi_point[\"aqi\"])\n",
        "                else:\n",
        "                    # Each step is a bit less certain (increasing randomness)\n",
        "                    prev = aqi_values[-1] if aqi_values else last_aqi\n",
        "                    random_component = np.random.normal(0, 2 + i)\n",
        "                    aqi_values.append(max(0, prev * 0.9 + random_component))\n",
        "                    \n",
        "        else:  # Default or any other model\n",
        "            # Simple linear trend with noise\n",
        "            for i in range(7):\n",
        "                if i == 0 and current_aqi_point:\n",
        "                    # For today, use the actual current AQI\n",
        "                    aqi_values.append(current_aqi_point[\"aqi\"])\n",
        "                else:\n",
        "                    base = last_aqi - i * 2  # Linear decrease\n",
        "                    noise = np.random.normal(0, 5)\n",
        "                    aqi_values.append(max(0, base + noise))\n",
        "        \n",
        "        # Round AQI values\n",
        "        forecast_df[\"aqi\"] = [round(val) for val in aqi_values]\n",
        "        \n",
        "        # Generate pollutant predictions\n",
        "        if \"pollutants\" in df.columns or any(col in df.columns for col in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]):\n",
        "            # Get the latest pollutant values as base\n",
        "            latest_pollutants = {}\n",
        "            for pollutant in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]:\n",
        "                if pollutant in df.columns:\n",
        "                    latest_pollutants[pollutant] = df.iloc[-1].get(pollutant, 0)\n",
        "                else:\n",
        "                    latest_pollutants[pollutant] = 0\n",
        "            \n",
        "            # Add predictions for each pollutant\n",
        "            for pollutant in [\"pm25\", \"pm10\", \"no2\", \"o3\", \"co\", \"so2\", \"nh3\"]:\n",
        "                base_val = latest_pollutants[pollutant]\n",
        "                pollutant_vals = []\n",
        "                \n",
        "                for i in range(7):\n",
        "                    if i == 0 and current_aqi_point and pollutant in current_aqi_point:\n",
        "                        # For today, use actual value if available\n",
        "                        pollutant_vals.append(current_aqi_point[pollutant])\n",
        "                    else:\n",
        "                        # Generate reasonable prediction based on base value and AQI trend\n",
        "                        aqi_ratio = aqi_values[i] / last_aqi if last_aqi > 0 else 1\n",
        "                        predicted_val = base_val * aqi_ratio * (0.95 + np.random.random() * 0.1)\n",
        "                        pollutant_vals.append(max(0, round(predicted_val)))\n",
        "                \n",
        "                forecast_df[pollutant] = pollutant_vals\n",
        "        \n",
        "        # Convert date column to string format\n",
        "        forecast_df[\"date\"] = forecast_df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating predictions: {str(e)}\")\n",
        "        # Return empty dataframe if prediction fails\n",
        "        return pd.DataFrame(columns=[\"date\", \"city\", \"location\", \"aqi\", \"predicted\"])\n",
        "    \n",
        "    return forecast_df\n",
        "\n",
        "# Run the server with: uvicorn main:app --reload\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "models_dir_cell"
      },
      "source": [
        "## 3. Create Model Directory\n",
        "\n",
        "Let's create a directory to store our machine learning models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_models_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dummy_models_cell"
      },
      "source": [
        "## 4. Create Dummy ML Models\n",
        "\n",
        "For demonstration purposes, let's create some dummy ML models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dummy_models"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a dummy ARIMA model\n",
        "model = LinearRegression()\n",
        "model.fit(np.array([[1], [2], [3]]), np.array([10, 20, 30]))\n",
        "joblib.dump(model, \"models/arima_model.pkl\")\n",
        "print(\"Created ARIMA model\")\n",
        "\n",
        "# Create a dummy LSTM model\n",
        "model = LinearRegression()\n",
        "model.fit(np.array([[1], [2], [3], [4]]), np.array([12, 22, 32, 42]))\n",
        "joblib.dump(model, \"models/lstm_model.pkl\")\n",
        "print(\"Created LSTM model\")\n",
        "\n",
        "# Create a dummy RandomForest model\n",
        "model = RandomForestRegressor(n_estimators=5)\n",
        "model.fit(np.array([[1], [2], [3], [4], [5]]), np.array([10, 20, 30, 40, 50]))\n",
        "joblib.dump(model, \"models/randomforest_model.pkl\")\n",
        "print(\"Created RandomForest model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_server_cell"
      },
      "source": [
        "## 5. Run the FastAPI Server\n",
        "\n",
        "Now, let's run our FastAPI server with ngrok to make it publicly accessible:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_ngrok"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_with_ngrok"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Apply nest_asyncio to allow running asyncio event loops in Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "# NOTE: To get an authtoken, sign up at https://ngrok.com/ and run: ngrok authtoken YOUR_TOKEN\n",
        "# If you don't provide an auth token, the connection will be limited\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "print(f\"\\nUse this URL to connect your frontend to the backend API\")\n",
        "print(f\"API documentation is available at {ngrok_tunnel.public_url}/docs\")\n",
        "\n",
        "# Run FastAPI in a separate thread\n",
        "def run_server():\n",
        "    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the server in a thread\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "# Keep the notebook running\n",
        "print(\"\\nServer is running. Keep this cell running to maintain the connection.\")\n",
        "print(\"To stop the server, interrupt the kernel (press ⏹ or select Kernel > Interrupt from the menu)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing_cell"
      },
      "source": [
        "## 6. Test the API\n",
        "\n",
        "Let's test the API with a simple request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_api"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Use localhost for testing within Colab\n",
        "base_url = \"http://localhost:8000\"\n",
        "\n",
        "# Test the root endpoint\n",
        "response = requests.get(f\"{base_url}/\")\n",
        "print(\"Root endpoint response:\")\n",
        "print(response.json())\n",
        "\n",
        "# Note: To test with actual API keys, you would need to provide them below\n",
        "# print(\"\\nYou can now use the public ngrok URL in your frontend application\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connecting_to_frontend_cell"
      },
      "source": [
        "## 7. Connect to Frontend\n",
        "\n",
        "To connect this backend to your AQI Prediction frontend:\n",
        "\n",
        "1. Copy the ngrok URL displayed above\n",
        "2. Create a new file in your frontend project to communicate with this API\n",
        "\n",
        "Here's sample frontend integration code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frontend_integration_code"
      },
      "outputs": [],
      "source": [
        "%%writefile backend_integration.js\n",
        "// This is a sample frontend integration file (for reference only)\n",
        "\n",
        "// Replace with your ngrok URL from above\n",
        "const BACKEND_URL = \"https://your-ngrok-url-here.ngrok.io\";\n",
        "\n",
        "// Function to fetch AQI data from the backend\n",
        "async function fetchAQIDataFromBackend(city, state, country, apiKey, platform) {\n",
        "  try {\n",
        "    const response = await fetch(`${BACKEND_URL}/api/fetch-aqi`, {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "      },\n",
        "      body: JSON.stringify({\n",
        "        city,\n",
        "        state,\n",
        "        country,\n",
        "        api_key: apiKey,\n",
        "        platform\n",
        "      }),\n",
        "    });\n",
        "    \n",
        "    if (!response.ok) {\n",
        "      throw new Error(`API error: ${response.status}`);\n",
        "    }\n",
        "    \n",
        "    return await response.json();\n",
        "  } catch (error) {\n",
        "    console.error('Error fetching data from backend:', error);\n",
        "    throw error;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Function to get predictions from the backend\n",
        "async function getPredictionsFromBackend(historicalData, modelName) {\n",
        "  try {\n",
        "    const response = await fetch(`${BACKEND_URL}/api/predict`, {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "      },\n",
        "      body: JSON.stringify({\n",
        "        historical_data: historicalData,\n",
        "        model_name: modelName\n",
        "      }),\n",
        "    });\n",
        "    \n",
        "    if (!response.ok) {\n",
        "      throw new Error(`API error: ${response.status}`);\n",
        "    }\n",
        "    \n",
        "    return await response.json();\n",
        "  } catch (error) {\n",
        "    console.error('Error getting predictions from backend:', error);\n",
        "    throw error;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Example usage in frontend\n",
        "async function fetchAndPredict(city, state, apiKey, platform, modelName) {\n",
        "  try {\n",
        "    // 1. Fetch historical AQI data\n",
        "    const historicalData = await fetchAQIDataFromBackend(city, state, 'India', apiKey, platform);\n",
        "    \n",
        "    // 2. Get predictions using the model\n",
        "    const predictions = await getPredictionsFromBackend(historicalData, modelName);\n",
        "    \n",
        "    // 3. Return combined results\n",
        "    return {\n",
        "      historicalData,\n",
        "      predictions\n",
        "    };\n",
        "  } catch (error) {\n",
        "    console.error('Error in fetch and predict workflow:', error);\n",
        "    throw error;\n",
        "  }\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "AQI Prediction Backend",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
